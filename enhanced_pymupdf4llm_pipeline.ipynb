{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9aadb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Core PDF processing libraries\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "import ocrmypdf\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bb2925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enhanced Metadata Extraction Functions\n",
    "\n",
    "def extract_enhanced_metadata(pdf_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract comprehensive metadata from PDF including academic paper information.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing enhanced metadata\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"title\": None,\n",
    "        \"authors\": [],\n",
    "        \"journal\": None,\n",
    "        \"date_published\": None,\n",
    "        \"doi\": None,\n",
    "        \"abstract\": None,\n",
    "        \"keywords\": [],\n",
    "        \"page_count\": 0,\n",
    "        \"language\": \"en\",\n",
    "        \"subject\": None,\n",
    "        \"creation_date\": None,\n",
    "        \"modification_date\": None,\n",
    "        \"creator\": None,\n",
    "        \"producer\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        \n",
    "        # Extract basic PDF metadata\n",
    "        pdf_metadata = doc.metadata\n",
    "        metadata[\"page_count\"] = len(doc)\n",
    "        \n",
    "        if pdf_metadata:\n",
    "            metadata[\"title\"] = pdf_metadata.get(\"title\", \"\").strip()\n",
    "            metadata[\"subject\"] = pdf_metadata.get(\"subject\", \"\").strip()\n",
    "            metadata[\"creation_date\"] = pdf_metadata.get(\"creationDate\", \"\")\n",
    "            metadata[\"modification_date\"] = pdf_metadata.get(\"modDate\", \"\")\n",
    "            metadata[\"creator\"] = pdf_metadata.get(\"creator\", \"\")\n",
    "            metadata[\"producer\"] = pdf_metadata.get(\"producer\", \"\")\n",
    "            \n",
    "            # Extract keywords from metadata\n",
    "            keywords_str = pdf_metadata.get(\"keywords\", \"\")\n",
    "            if keywords_str:\n",
    "                metadata[\"keywords\"] = [kw.strip() for kw in keywords_str.split(\",\") if kw.strip()]\n",
    "        \n",
    "        # Extract text from first few pages for content analysis\n",
    "        first_page_text = \"\"\n",
    "        for page_num in range(min(3, len(doc))):\n",
    "            page = doc[page_num]\n",
    "            first_page_text += page.get_text() + \"\\n\"\n",
    "        \n",
    "        # Enhanced content-based metadata extraction\n",
    "        metadata.update(extract_academic_metadata(first_page_text))\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Clean up extracted data\n",
    "        metadata = clean_metadata(metadata)\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting metadata: {e}\")\n",
    "        return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31a25c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_academic_metadata(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract academic paper-specific metadata from text content.\n",
    "    \n",
    "    Args:\n",
    "        text: Text content from PDF\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing academic metadata\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"title\": None,\n",
    "        \"authors\": [],\n",
    "        \"journal\": None,\n",
    "        \"date_published\": None,\n",
    "        \"doi\": None,\n",
    "        \"abstract\": None,\n",
    "        \"keywords\": []\n",
    "    }\n",
    "    \n",
    "    # Extract DOI\n",
    "    doi_pattern = r'(?:DOI|doi)[\\s:]*([0-9]{2}\\.[0-9]{4}\\/[^\\s]+)'\n",
    "    doi_match = re.search(doi_pattern, text, re.IGNORECASE)\n",
    "    if doi_match:\n",
    "        metadata[\"doi\"] = doi_match.group(1)\n",
    "    \n",
    "    # Extract title (usually the largest text on first page)\n",
    "    title_patterns = [\n",
    "        r'^([A-Z][^.!?]*(?:[.!?][A-Z][^.!?]*)*)\\n',  # Title at start\n",
    "        r'\\n([A-Z][^.!?]*(?:[.!?][A-Z][^.!?]*)*)\\n(?=\\n|[A-Z])',  # Title with newlines\n",
    "        r'(?:Title|TITLE)[\\s:]*([^\\n]+)',  # Explicit title\n",
    "    ]\n",
    "    \n",
    "    for pattern in title_patterns:\n",
    "        match = re.search(pattern, text, re.MULTILINE)\n",
    "        if match:\n",
    "            potential_title = match.group(1).strip()\n",
    "            if 10 < len(potential_title) < 200:  # Reasonable title length\n",
    "                metadata[\"title\"] = potential_title\n",
    "                break\n",
    "    \n",
    "    # Extract authors\n",
    "    author_patterns = [\n",
    "        r'(?:Authors?|BY)[\\s:]*([^\\n]+)',\n",
    "        r'([A-Z][a-z]+ [A-Z][a-z]+(?:,\\s*[A-Z][a-z]+ [A-Z][a-z]+)*)',\n",
    "        r'([A-Z]\\.\\s*[A-Z][a-z]+(?:,\\s*[A-Z]\\.\\s*[A-Z][a-z]+)*)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in author_patterns:\n",
    "        matches = re.findall(pattern, text, re.MULTILINE)\n",
    "        if matches:\n",
    "            for author_string in matches:\n",
    "                authors = [author.strip() for author in author_string.split(',')]\n",
    "                metadata[\"authors\"].extend(authors)\n",
    "            break\n",
    "    \n",
    "    # Extract journal information\n",
    "    journal_patterns = [\n",
    "        r'(?:Journal|Proceedings|Conference)[\\s:]*([^\\n]+)',\n",
    "        r'Published in[\\s:]*([^\\n]+)',\n",
    "        r'([A-Z][a-z]+ Journal[^\\n]*)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in journal_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            metadata[\"journal\"] = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # Extract publication date\n",
    "    date_patterns = [\n",
    "        r'(?:Published|Publication date|Date)[\\s:]*([0-9]{4})',\n",
    "        r'([0-9]{1,2}[\\s/-][0-9]{1,2}[\\s/-][0-9]{4})',\n",
    "        r'([A-Z][a-z]+ [0-9]{1,2}, [0-9]{4})',\n",
    "        r'([0-9]{4})',\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            metadata[\"date_published\"] = match.group(1)\n",
    "            break\n",
    "    \n",
    "    # Extract abstract\n",
    "    abstract_pattern = r'(?:Abstract|ABSTRACT)[\\s:]*\\n?(.*?)(?=\\n\\n|Keywords|Introduction|1\\.|\\n[A-Z])'\n",
    "    abstract_match = re.search(abstract_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "    if abstract_match:\n",
    "        abstract_text = abstract_match.group(1).strip()\n",
    "        if len(abstract_text) > 50:  # Reasonable abstract length\n",
    "            metadata[\"abstract\"] = abstract_text\n",
    "    \n",
    "    # Extract keywords\n",
    "    keywords_pattern = r'(?:Keywords?|Key words?)[\\s:]*([^\\n]+)'\n",
    "    keywords_match = re.search(keywords_pattern, text, re.IGNORECASE)\n",
    "    if keywords_match:\n",
    "        keywords_str = keywords_match.group(1)\n",
    "        keywords = [kw.strip() for kw in re.split(r'[,;]', keywords_str) if kw.strip()]\n",
    "        metadata[\"keywords\"] = keywords\n",
    "    \n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f5f1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Clean and normalize extracted metadata.\n",
    "    \n",
    "    Args:\n",
    "        metadata: Raw metadata dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned metadata dictionary\n",
    "    \"\"\"\n",
    "    # Clean title\n",
    "    if metadata.get(\"title\"):\n",
    "        title = metadata[\"title\"]\n",
    "        # Remove excessive whitespace\n",
    "        title = re.sub(r'\\s+', ' ', title).strip()\n",
    "        # Remove common artifacts\n",
    "        title = re.sub(r'^(Title|TITLE)[\\s:]*', '', title)\n",
    "        metadata[\"title\"] = title\n",
    "    \n",
    "    # Clean authors\n",
    "    if metadata.get(\"authors\"):\n",
    "        cleaned_authors = []\n",
    "        for author in metadata[\"authors\"]:\n",
    "            author = author.strip()\n",
    "            if author and len(author) > 2:\n",
    "                # Remove common artifacts\n",
    "                author = re.sub(r'[0-9*,]+$', '', author).strip()\n",
    "                if author:\n",
    "                    cleaned_authors.append(author)\n",
    "        metadata[\"authors\"] = cleaned_authors[:10]  # Limit to reasonable number\n",
    "    \n",
    "    # Clean journal\n",
    "    if metadata.get(\"journal\"):\n",
    "        journal = metadata[\"journal\"]\n",
    "        journal = re.sub(r'\\s+', ' ', journal).strip()\n",
    "        metadata[\"journal\"] = journal\n",
    "    \n",
    "    # Clean keywords\n",
    "    if metadata.get(\"keywords\"):\n",
    "        cleaned_keywords = []\n",
    "        for keyword in metadata[\"keywords\"]:\n",
    "            keyword = keyword.strip()\n",
    "            if keyword and len(keyword) > 1:\n",
    "                cleaned_keywords.append(keyword)\n",
    "        metadata[\"keywords\"] = cleaned_keywords[:20]  # Limit to reasonable number\n",
    "    \n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47e8f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enhanced Content Formatting Functions\n",
    "\n",
    "def format_enhanced_markdown(extracted_content: Any, metadata: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format extracted content into properly structured markdown with hierarchy.\n",
    "    \n",
    "    Args:\n",
    "        extracted_content: Raw content from PyMuPDF4LLM\n",
    "        metadata: Enhanced metadata\n",
    "        \n",
    "    Returns:\n",
    "        Formatted markdown string\n",
    "    \"\"\"\n",
    "    markdown_parts = []\n",
    "    \n",
    "    # Add title\n",
    "    if metadata.get(\"title\"):\n",
    "        markdown_parts.append(f\"# {metadata['title']}\\n\")\n",
    "    \n",
    "    # Add metadata section\n",
    "    metadata_section = format_metadata_section(metadata)\n",
    "    if metadata_section:\n",
    "        markdown_parts.append(metadata_section)\n",
    "    \n",
    "    # Process and format main content\n",
    "    content_str = str(extracted_content) if extracted_content else \"\"\n",
    "    \n",
    "    # Clean up the content\n",
    "    content_str = clean_content_text(content_str)\n",
    "    \n",
    "    # Structure the content with proper headers\n",
    "    structured_content = structure_content_with_headers(content_str)\n",
    "    \n",
    "    markdown_parts.append(structured_content)\n",
    "    \n",
    "    return \"\\n\".join(markdown_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41bec498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata_section(metadata: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format metadata into a structured markdown section.\n",
    "    \n",
    "    Args:\n",
    "        metadata: Metadata dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Formatted metadata section\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    \n",
    "    # Authors\n",
    "    if metadata.get(\"authors\"):\n",
    "        authors_str = \", \".join(metadata[\"authors\"])\n",
    "        sections.append(f\"**Authors:** {authors_str}\")\n",
    "    \n",
    "    # Journal\n",
    "    if metadata.get(\"journal\"):\n",
    "        sections.append(f\"**Journal:** {metadata['journal']}\")\n",
    "    \n",
    "    # Publication date\n",
    "    if metadata.get(\"date_published\"):\n",
    "        sections.append(f\"**Published:** {metadata['date_published']}\")\n",
    "    \n",
    "    # DOI\n",
    "    if metadata.get(\"doi\"):\n",
    "        sections.append(f\"**DOI:** {metadata['doi']}\")\n",
    "    \n",
    "    # Keywords\n",
    "    if metadata.get(\"keywords\"):\n",
    "        keywords_str = \", \".join(metadata[\"keywords\"])\n",
    "        sections.append(f\"**Keywords:** {keywords_str}\")\n",
    "    \n",
    "    # Page count\n",
    "    if metadata.get(\"page_count\"):\n",
    "        sections.append(f\"**Pages:** {metadata['page_count']}\")\n",
    "    \n",
    "    if sections:\n",
    "        return \"## Document Information\\n\\n\" + \"\\n\\n\".join(sections) + \"\\n\\n---\\n\\n\"\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "383cfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content_text(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize content text.\n",
    "    \n",
    "    Args:\n",
    "        content: Raw content string\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned content string\n",
    "    \"\"\"\n",
    "    # Remove metadata dictionary if present\n",
    "    if content.startswith(\"{'metadata'\"):\n",
    "        # Find the end of the metadata dictionary\n",
    "        bracket_count = 0\n",
    "        in_string = False\n",
    "        escape_next = False\n",
    "        \n",
    "        for i, char in enumerate(content):\n",
    "            if escape_next:\n",
    "                escape_next = False\n",
    "                continue\n",
    "            \n",
    "            if char == '\\\\':\n",
    "                escape_next = True\n",
    "                continue\n",
    "            \n",
    "            if char == '\"' and not escape_next:\n",
    "                in_string = not in_string\n",
    "            \n",
    "            if not in_string:\n",
    "                if char == '{':\n",
    "                    bracket_count += 1\n",
    "                elif char == '}':\n",
    "                    bracket_count -= 1\n",
    "                    if bracket_count == 0:\n",
    "                        content = content[i+1:].strip()\n",
    "                        break\n",
    "    \n",
    "    # Clean up excessive whitespace\n",
    "    content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
    "    content = re.sub(r' +', ' ', content)\n",
    "    \n",
    "    # Remove page numbers and headers/footers\n",
    "    lines = content.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Skip obvious page numbers\n",
    "        if re.match(r'^\\d+$', line):\n",
    "            continue\n",
    "        # Skip short lines that are likely headers/footers\n",
    "        if len(line) < 5:\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a23ed097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_content_with_headers(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Add proper markdown headers to structure the content.\n",
    "    \n",
    "    Args:\n",
    "        content: Cleaned content string\n",
    "        \n",
    "    Returns:\n",
    "        Content with proper markdown headers\n",
    "    \"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    structured_lines = []\n",
    "    \n",
    "    # Common section headers in academic papers\n",
    "    section_patterns = [\n",
    "        (r'^(Abstract|ABSTRACT)$', '## Abstract'),\n",
    "        (r'^(Introduction|INTRODUCTION)$', '## Introduction'),\n",
    "        (r'^(Background|BACKGROUND)$', '## Background'),\n",
    "        (r'^(Methods?|METHODS?)$', '## Methods'),\n",
    "        (r'^(Results?|RESULTS?)$', '## Results'),\n",
    "        (r'^(Discussion|DISCUSSION)$', '## Discussion'),\n",
    "        (r'^(Conclusion|CONCLUSION|Conclusions|CONCLUSIONS)$', '## Conclusion'),\n",
    "        (r'^(References?|REFERENCES?)$', '## References'),\n",
    "        (r'^(Acknowledgments?|ACKNOWLEDGMENTS?)$', '## Acknowledgments'),\n",
    "        (r'^(Appendix|APPENDIX)$', '## Appendix'),\n",
    "        (r'^(\\d+\\.\\s*[A-Z][^.]*?)$', '## \\\\1'),  # Numbered sections\n",
    "        (r'^([A-Z][A-Z\\s]{3,}[A-Z])$', '## \\\\1'),  # ALL CAPS headers\n",
    "    ]\n",
    "    \n",
    "    subsection_patterns = [\n",
    "        (r'^(\\d+\\.\\d+\\s*[A-Z][^.]*?)$', '### \\\\1'),  # Numbered subsections\n",
    "        (r'^([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*):$', '### \\\\1'),  # Title case with colon\n",
    "    ]\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            structured_lines.append('')\n",
    "            continue\n",
    "        \n",
    "        # Check for main section headers\n",
    "        header_found = False\n",
    "        for pattern, replacement in section_patterns:\n",
    "            if re.match(pattern, line):\n",
    "                structured_lines.append(f'\\n{replacement}\\n')\n",
    "                header_found = True\n",
    "                break\n",
    "        \n",
    "        if not header_found:\n",
    "            # Check for subsection headers\n",
    "            for pattern, replacement in subsection_patterns:\n",
    "                if re.match(pattern, line):\n",
    "                    structured_lines.append(f'\\n{replacement}\\n')\n",
    "                    header_found = True\n",
    "                    break\n",
    "        \n",
    "        if not header_found:\n",
    "            # Check if line looks like a header (bold, larger font, etc.)\n",
    "            if is_likely_header(line):\n",
    "                level = determine_header_level(line)\n",
    "                header_prefix = '#' * level\n",
    "                structured_lines.append(f'\\n{header_prefix} {line}\\n')\n",
    "            else:\n",
    "                structured_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(structured_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ff375d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_likely_header(line: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a line is likely a header based on various criteria.\n",
    "    \n",
    "    Args:\n",
    "        line: Text line to analyze\n",
    "        \n",
    "    Returns:\n",
    "        True if line is likely a header\n",
    "    \"\"\"\n",
    "    # Check for bold formatting\n",
    "    if line.startswith('**') and line.endswith('**'):\n",
    "        return True\n",
    "    \n",
    "    # Check for title case\n",
    "    if line.istitle() and len(line.split()) <= 8:\n",
    "        return True\n",
    "    \n",
    "    # Check for all caps (but not too long)\n",
    "    if line.isupper() and 5 <= len(line) <= 50:\n",
    "        return True\n",
    "    \n",
    "    # Check for numbered headers\n",
    "    if re.match(r'^\\d+\\.?\\s+[A-Z]', line):\n",
    "        return True\n",
    "    \n",
    "    # Check for common header patterns\n",
    "    header_indicators = [\n",
    "        'Background:', 'Methods:', 'Results:', 'Discussion:', 'Conclusion:',\n",
    "        'Introduction:', 'Abstract:', 'References:', 'Acknowledgments:'\n",
    "    ]\n",
    "    \n",
    "    for indicator in header_indicators:\n",
    "        if line.startswith(indicator):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5137050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_header_level(line: str) -> int:\n",
    "    \"\"\"\n",
    "    Determine the appropriate header level for a line.\n",
    "    \n",
    "    Args:\n",
    "        line: Header line text\n",
    "        \n",
    "    Returns:\n",
    "        Header level (2-4)\n",
    "    \"\"\"\n",
    "    # Main section headers\n",
    "    main_sections = [\n",
    "        'abstract', 'introduction', 'background', 'methods', 'results',\n",
    "        'discussion', 'conclusion', 'references', 'acknowledgments'\n",
    "    ]\n",
    "    \n",
    "    if any(section in line.lower() for section in main_sections):\n",
    "        return 2\n",
    "    \n",
    "    # Numbered sections\n",
    "    if re.match(r'^\\d+\\.?\\s+', line):\n",
    "        return 2\n",
    "    \n",
    "    # Numbered subsections\n",
    "    if re.match(r'^\\d+\\.\\d+', line):\n",
    "        return 3\n",
    "    \n",
    "    # Default to level 3 for other headers\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6588d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enhanced Main Processing Pipeline\n",
    "\n",
    "# %%\n",
    "def process_pdf_enhanced(input_path: str, output_dir: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced PDF processing pipeline with better formatting and metadata.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input PDF file\n",
    "        output_dir: Directory for output files (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing processing results and metadata\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {input_path}\")\n",
    "    \n",
    "    # Setup output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = Path(input_path).parent / \"enhanced_processed\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        \"input_path\": input_path,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"processing_start\": start_time,\n",
    "        \"pdf_type\": None,\n",
    "        \"has_toc\": False,\n",
    "        \"ocr_applied\": False,\n",
    "        \"processed_pdf_path\": None,\n",
    "        \"extraction_method\": None,\n",
    "        \"enhanced_metadata\": {},\n",
    "        \"formatted_content\": None,\n",
    "        \"output_files\": {},\n",
    "        \"processing_time\": None,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Starting enhanced PDF processing: {input_path}\")\n",
    "        \n",
    "        # Step 1: Extract enhanced metadata\n",
    "        logger.info(\"Step 1: Extracting enhanced metadata...\")\n",
    "        enhanced_metadata = extract_enhanced_metadata(input_path)\n",
    "        results[\"enhanced_metadata\"] = enhanced_metadata\n",
    "        \n",
    "        # Step 2: Analyze PDF type\n",
    "        logger.info(\"Step 2: Analyzing PDF type...\")\n",
    "        is_scanned = is_pdf_scanned(input_path)\n",
    "        has_toc = has_table_of_contents(input_path)\n",
    "        \n",
    "        results[\"pdf_type\"] = \"scanned\" if is_scanned else \"born_digital\"\n",
    "        results[\"has_toc\"] = has_toc\n",
    "        \n",
    "        # Step 3: Preprocess if needed\n",
    "        processed_pdf_path = input_path\n",
    "        \n",
    "        if is_scanned:\n",
    "            logger.info(\"Step 3: Applying OCRmyPDF preprocessing...\")\n",
    "            base_name = Path(input_path).stem\n",
    "            ocr_output_path = os.path.join(output_dir, f\"{base_name}_ocr_processed.pdf\")\n",
    "            processed_pdf_path = preprocess_with_ocrmypdf(input_path, ocr_output_path)\n",
    "            results[\"ocr_applied\"] = True\n",
    "            results[\"processed_pdf_path\"] = processed_pdf_path\n",
    "            \n",
    "            # Re-check TOC after OCR\n",
    "            has_toc = has_table_of_contents(processed_pdf_path)\n",
    "            results[\"has_toc\"] = has_toc\n",
    "        \n",
    "        # Step 4: Extract content with PyMuPDF4LLM\n",
    "        logger.info(\"Step 4: Extracting content with PyMuPDF4LLM...\")\n",
    "        extraction_start = time.time()\n",
    "        \n",
    "        # Configure PyMuPDF4LLM parameters\n",
    "        pymupdf_params = {\n",
    "            \"page_chunks\": True,\n",
    "            \"write_images\": True,\n",
    "            \"image_path\": os.path.join(output_dir, \"images\"),\n",
    "            \"image_format\": \"png\",\n",
    "        }\n",
    "        \n",
    "        # Extract content\n",
    "        extracted_content = pymupdf4llm.to_markdown(\n",
    "            processed_pdf_path,\n",
    "            **pymupdf_params\n",
    "        )\n",
    "        \n",
    "        extraction_time = time.time() - extraction_start\n",
    "        results[\"extraction_method\"] = \"enhanced_pymupdf4llm\"\n",
    "        \n",
    "        # Step 5: Format content with enhanced structure\n",
    "        logger.info(\"Step 5: Formatting content with enhanced structure...\")\n",
    "        formatted_content = format_enhanced_markdown(extracted_content, enhanced_metadata)\n",
    "        results[\"formatted_content\"] = formatted_content\n",
    "        \n",
    "        # Step 6: Save all outputs\n",
    "        logger.info(\"Step 6: Saving enhanced outputs...\")\n",
    "        base_name = Path(input_path).stem\n",
    "        \n",
    "        # Save enhanced markdown\n",
    "        markdown_path = os.path.join(output_dir, f\"{base_name}_enhanced.md\")\n",
    "        with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(formatted_content)\n",
    "        \n",
    "        # Save enhanced metadata\n",
    "        metadata_path = os.path.join(output_dir, f\"{base_name}_enhanced_metadata.json\")\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(enhanced_metadata, f, indent=2, default=str)\n",
    "        \n",
    "        # Save processing report\n",
    "        report_path = os.path.join(output_dir, f\"{base_name}_processing_report.json\")\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            report = {k: v for k, v in results.items() if k != 'formatted_content'}\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        results[\"output_files\"] = {\n",
    "            \"markdown\": markdown_path,\n",
    "            \"metadata\": metadata_path,\n",
    "            \"report\": report_path\n",
    "        }\n",
    "        \n",
    "        # Calculate total processing time\n",
    "        total_time = time.time() - start_time\n",
    "        results[\"processing_time\"] = total_time\n",
    "        \n",
    "        logger.info(f\"Enhanced processing completed successfully in {total_time:.2f} seconds\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Enhanced processing failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        results[\"errors\"].append(error_msg)\n",
    "        results[\"processing_time\"] = time.time() - start_time\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5115011",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Functions \n",
    "def is_pdf_scanned(pdf_path: str, text_threshold: float = 0.1) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if a PDF is scanned (image-based) or born-digital.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_chars = 0\n",
    "        total_area = 0\n",
    "        \n",
    "        sample_pages = min(5, len(doc))\n",
    "        \n",
    "        for page_num in range(sample_pages):\n",
    "            page = doc[page_num]\n",
    "            text = page.get_text()\n",
    "            total_chars += len(text.strip())\n",
    "            total_area += page.rect.width * page.rect.height\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        if total_area == 0:\n",
    "            return True\n",
    "            \n",
    "        text_density = total_chars / total_area\n",
    "        is_scanned = text_density < text_threshold\n",
    "        \n",
    "        logger.info(f\"PDF analysis: {total_chars} chars, density: {text_density:.6f}, scanned: {is_scanned}\")\n",
    "        return is_scanned\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing PDF: {e}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95da7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_table_of_contents(pdf_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if PDF has an embedded table of contents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        toc = doc.get_toc()\n",
    "        doc.close()\n",
    "        return len(toc) > 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking TOC: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4983d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_ocrmypdf(input_path: str, output_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess PDF with OCRmyPDF to add OCR layer and fix orientation.\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        base_name = Path(input_path).stem\n",
    "        output_path = os.path.join(temp_dir, f\"{base_name}_ocr_processed.pdf\")\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Starting OCRmyPDF preprocessing: {input_path}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        ocrmypdf.ocr(\n",
    "            input_path,\n",
    "            output_path,\n",
    "            language=['eng'],\n",
    "            rotate_pages=True,\n",
    "            deskew=True,\n",
    "            clean=True,\n",
    "            optimize=1,\n",
    "            pdf_renderer='hocr',\n",
    "            force_ocr=False,\n",
    "            skip_text=False,\n",
    "            redo_ocr=False,\n",
    "            progress_bar=False,\n",
    "            quiet=True\n",
    "        )\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"OCRmyPDF completed in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"OCRmyPDF preprocessing failed: {e}\")\n",
    "        return input_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a4963ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting enhanced PDF processing: pdfs/Allossogbe_et_al_2017_Mal_J.pdf\n",
      "INFO:__main__:Step 1: Extracting enhanced metadata...\n",
      "INFO:__main__:Step 2: Analyzing PDF type...\n",
      "INFO:__main__:PDF analysis: 19485 chars, density: 0.008278, scanned: True\n",
      "INFO:__main__:Step 3: Applying OCRmyPDF preprocessing...\n",
      "INFO:__main__:Starting OCRmyPDF preprocessing: pdfs/Allossogbe_et_al_2017_Mal_J.pdf\n",
      "WARNING:ocrmypdf.subprocess._windows:[WinError 2] The system cannot find the file specified\n",
      "ERROR:ocrmypdf.subprocess:\n",
      "The program 'unpaper' could not be executed or was not found on your\n",
      "system PATH.  This program is required when you use the\n",
      "--clean, --clean-final arguments.  You could try omitting these arguments, or install\n",
      "the package.\n",
      "\n",
      "INFO:ocrmypdf.subprocess:\n",
      "If not already installed, install the Chocolatey package manager. Then use\n",
      "a command prompt to install the missing package:\n",
      "    choco install unpaper\n",
      "\n",
      "ERROR:__main__:OCRmyPDF preprocessing failed: Could not find program 'unpaper' on the PATH\n",
      "INFO:__main__:Step 4: Extracting content with PyMuPDF4LLM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced PDF Processing ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 5: Formatting content with enhanced structure...\n",
      "INFO:__main__:Step 6: Saving enhanced outputs...\n",
      "INFO:__main__:Enhanced processing completed successfully in 8.43 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processing completed!\n",
      "üìÑ PDF Type: scanned\n",
      "üìö Has TOC: True\n",
      "üîç OCR Applied: True\n",
      "‚è±Ô∏è Processing Time: 8.43 seconds\n",
      "\n",
      "üìã Enhanced Metadata:\n",
      "  ‚Ä¢ Title: None\n",
      "  ‚Ä¢ Authors: (s) 2017. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License\n",
      "  ‚Ä¢ Journal: *Correspondence: palutec@yahoo.fr\n",
      "  ‚Ä¢ Date: 2017\n",
      "  ‚Ä¢ DOI: 10.1186/s12936-017-1727-x\n",
      "  ‚Ä¢ Keywords: LLINs, Bio-eÔ¨Écacy, Piperonyl butoxide, Resistant mosquitoes\n",
      "\n",
      "üìÅ Output Files:\n",
      "  ‚Ä¢ Markdown: pdfs\\enhanced_processed\\Allossogbe_et_al_2017_Mal_J_enhanced.md\n",
      "  ‚Ä¢ Metadata: pdfs\\enhanced_processed\\Allossogbe_et_al_2017_Mal_J_enhanced_metadata.json\n",
      "  ‚Ä¢ Report: pdfs\\enhanced_processed\\Allossogbe_et_al_2017_Mal_J_processing_report.json\n"
     ]
    }
   ],
   "source": [
    "## Usage\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage of the enhanced PDF processing pipeline.\n",
    "    \"\"\"\n",
    "    # Example: Process a single PDF with enhanced features\n",
    "    pdf_path = \"pdfs/Allossogbe_et_al_2017_Mal_J.pdf\"\n",
    "    \n",
    "    if os.path.exists(pdf_path):\n",
    "        try:\n",
    "            print(\"=== Enhanced PDF Processing ===\")\n",
    "            results = process_pdf_enhanced(pdf_path)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Processing completed!\")\n",
    "            print(f\"üìÑ PDF Type: {results['pdf_type']}\")\n",
    "            print(f\"üìö Has TOC: {results['has_toc']}\")\n",
    "            print(f\"üîç OCR Applied: {results['ocr_applied']}\")\n",
    "            print(f\"‚è±Ô∏è Processing Time: {results['processing_time']:.2f} seconds\")\n",
    "            \n",
    "            # Display enhanced metadata\n",
    "            metadata = results['enhanced_metadata']\n",
    "            print(f\"\\nüìã Enhanced Metadata:\")\n",
    "            print(f\"  ‚Ä¢ Title: {metadata.get('title', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Authors: {', '.join(metadata.get('authors', []))}\")\n",
    "            print(f\"  ‚Ä¢ Journal: {metadata.get('journal', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Date: {metadata.get('date_published', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ DOI: {metadata.get('doi', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Keywords: {', '.join(metadata.get('keywords', []))}\")\n",
    "            \n",
    "            # Display output files\n",
    "            print(f\"\\nüìÅ Output Files:\")\n",
    "            for file_type, path in results['output_files'].items():\n",
    "                print(f\"  ‚Ä¢ {file_type.capitalize()}: {path}\")\n",
    "            \n",
    "            if results['errors']:\n",
    "                print(f\"\\n‚ùå Errors: {results['errors']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing PDF: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ùå PDF file not found: {pdf_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
