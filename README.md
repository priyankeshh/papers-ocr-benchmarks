# OCR Benchmarking System for Scientific Literature

This repository contains a comprehensive benchmarking framework that evaluates and compares OCR systems on scientific literature. The project successfully benchmarks **3 OCR systems** (Marker, Docling, PyMuPDF) with **Marker achieving 82.6% accuracy** as the best performer.

## ğŸ¯ Project Status: âœ… COMPLETE

**Deliverables Completed:**
- âœ… **3 OCR systems benchmarked** (Marker + 2 others as requested)
- âœ… **Google Colab Notebook** with findings and analysis
- âœ… **Comprehensive metrics** for scientific document processing
- âœ… **Processing time analysis** and performance comparison
- âœ… **Real scientific literature** used as test dataset

## ğŸ“Š Key Results

| OCR System | Character Accuracy | Processing Time | Best For |
|------------|-------------------|----------------|----------|
| **Marker** â­ | **82.6%** | 39s avg | **Best Overall Performance** |
| **Docling** | 81.3% | 91s avg | Scientific Content Analysis |
| **PyMuPDF** | Baseline | <1s | Speed Baseline |

## ğŸ” Evaluation Metrics

The benchmarking system evaluates OCR systems across multiple dimensions:

### **Content Accuracy Metrics**
- **Text Extraction Accuracy**: Character, word, and line-level accuracy
- **Processing Performance**: Speed, efficiency, resource usage
- **Scientific Content Handling**: Formulas, notation, references, citations

### **Structure Parsing Metrics** ğŸ†•
- **Document Elements**: Title, authors, abstract detection
- **Section Organization**: Headers, paragraphs, reading order
- **Scientific Elements**: Equations, tables, figures, references
- **Layout Preservation**: Document hierarchy and structure

## ğŸ” OCR Systems Benchmarked

### **Successfully Tested âœ…**
- **Marker** â­ - Priority system (82.6% accuracy, 39s avg processing)
- **Docling** - IBM's document AI (81.3% accuracy, 91s avg processing)  
- **PyMuPDF** - Direct text extraction baseline (<1s processing)

**Result**: Marker demonstrates the best overall performance for scientific literature OCR, balancing high accuracy with reasonable processing speed.

## ğŸ“‚ Repository Structure

```
papers-ocr-benchmarks/
â”œâ”€â”€ ğŸ““ OCR_Benchmark_Scientific_Literature.ipynb  # ğŸ¯ Google Colab Notebook (MAIN DELIVERABLE)
â”œâ”€â”€ ğŸ“„ README.md                                  # Project documentation
â”œâ”€â”€ ğŸ“‹ requirements.txt                           # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“ scripts/                                   # Benchmark scripts
â”‚   â”œâ”€â”€ ocr_benchmark_gpu_optimized.py           # Main benchmark script
â”‚   â””â”€â”€ setup_gpu_environment.py                 # Environment setup
â”‚
â”œâ”€â”€ ğŸ“ pdfs/                                      # Test dataset (3 scientific papers)
â”‚   â”œâ”€â”€ 2014-Combining_organophosphate_treated_wall_linings...pdf
â”‚   â”œâ”€â”€ Allossogbe_et_al_2017_Mal_J.pdf
â”‚   â””â”€â”€ Somboon_et_al_1995_Trans_RSTMH.pdf
â”‚
â”œâ”€â”€ ğŸ“ results/                                   # Benchmark results
â”‚   â”œâ”€â”€ benchmark_results.csv                    # Detailed metrics
â”‚   â”œâ”€â”€ benchmark_summary.csv                    # Summary statistics
â”‚   â”œâ”€â”€ benchmark_visualization.png              # Performance charts
â”‚   â””â”€â”€ [individual_ocr_outputs].txt             # Raw OCR extractions
â”‚
â”œâ”€â”€ ğŸ“ docs/                                      # Project documentation
â”‚   â””â”€â”€ Enhanced AI OCR Extraction Pipeline for Scientific Literature.md
â”‚
â””â”€â”€ ğŸ“ output/                                    # Generated results directory
```

## ğŸš€ Quick Start

### **For Review (Recommended)**
1. **View Results**: Open `OCR_Benchmark_Scientific_Literature.ipynb` in Google Colab
2. **Check Metrics**: Review `results/benchmark_summary.csv`
3. **Inspect Outputs**: Examine individual OCR outputs in `results/`

### **To Run Benchmark Locally**
```bash
# 1. Setup environment
python scripts/setup_gpu_environment.py

# 2. Run benchmark
python scripts/ocr_benchmark_gpu_optimized.py

# 3. Analyze document structure (NEW)
python scripts/structure_parser.py

# 4. Check results in results/ and examples/outputs/ directories
```

## ğŸ“Š Key Files

### **Main Deliverables**
- **`OCR_Benchmark_Scientific_Literature.ipynb`** - Google Colab notebook with complete analysis
- **`results/latest_benchmark_results.csv`** - Latest benchmark results summary
- **`examples/outputs/structure_comparison.csv`** - Structure parsing analysis
- **`STRUCTURE_ANALYSIS_REPORT.md`** - Detailed structure parsing evaluation

### **Scripts**
- **`scripts/ocr_benchmark_gpu_optimized.py`** - Main benchmark script with GPU optimization
- **`scripts/structure_parser.py`** - Document structure analysis tool
- **`scripts/setup_gpu_environment.py`** - Environment setup and dependency checking

### **Data & Results**
- **`pdfs/`** - Test dataset of 3 scientific papers
- **`results/`** - Complete benchmark results and individual OCR outputs
- **`examples/outputs/`** - Structured JSON outputs for each OCR system

## ğŸ“ˆ Benchmark Results Summary

### **Performance Metrics**
- **Best Accuracy**: Marker (82.6% character accuracy)
- **Fastest Processing**: PyMuPDF (<1s per document)
- **Best Balance**: Marker (high accuracy + reasonable speed)

### **Scientific Content Analysis**
- **Citations Detected**: All systems successfully identify reference citations
- **Figures/Tables**: Good preservation of figure and table references
- **Mathematical Content**: Basic formula detection implemented

### **Processing Time Analysis**
- **Marker**: 39s average (best AI-based performance)
- **Docling**: 91s average (thorough but slower)
- **PyMuPDF**: <1s (direct text extraction baseline)

## ğŸ”§ Technical Details

### **Dependencies**
```bash
pip install -r requirements.txt
```

Key packages:
- `marker-pdf` - Marker OCR system
- `docling` - IBM Docling system
- `PyMuPDF` - PDF text extraction
- `pandas`, `numpy` - Data analysis
- `torch` - GPU acceleration (optional)

### **System Requirements**
- Python 3.8+
- 8GB+ RAM recommended
- GPU optional (automatic CPU fallback)
- 5GB+ storage for results

